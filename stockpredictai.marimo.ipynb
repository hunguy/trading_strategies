{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Hbol",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Import required libraries for data pipeline\"\"\"\n",
    "import vectorbt as vbt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Tuple, Optional\n",
    "from datetime import datetime, timedelta\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MJUe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Download and save historical data from Yahoo Finance\"\"\"\n",
    "def download_stock_data(\n",
    "    symbol: str,\n",
    "    start_date: str = '2010-01-01',\n",
    "    end_date: Optional[str] = None,\n",
    "    interval: str = '1d'\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Download historical stock data from Yahoo Finance\n",
    "\n",
    "    Args:\n",
    "        symbol: Stock symbol (e.g., 'GS' for Goldman Sachs)\n",
    "        start_date: Start date in 'YYYY-MM-DD' format\n",
    "        end_date: End date in 'YYYY-MM-DD' format (defaults to current date)\n",
    "        interval: Data interval ('1d' for daily, '1h' for hourly)\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with OHLCV data\n",
    "    \"\"\"\n",
    "    if end_date is None:\n",
    "        end_date = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "    data = vbt.YFData.download(\n",
    "        symbol,\n",
    "        start=start_date,\n",
    "        end=end_date,\n",
    "        interval=interval,\n",
    "        missing_index='drop'\n",
    "    ).get([\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"])\n",
    "\n",
    "    return data\n",
    "\n",
    "# Example usage\n",
    "data = download_stock_data(\"GS\")\n",
    "data.to_csv(\"GS.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vblA",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Preprocess the data and create technical indicators\"\"\"\n",
    "def preprocess_data(\n",
    "    df: pd.DataFrame,\n",
    "    sequence_length: int = 60\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Preprocess the data by:\n",
    "    1. Handling missing values\n",
    "    2. Creating technical indicators\n",
    "    3. Normalizing the data\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame with OHLCV data\n",
    "        sequence_length: Length of sequences for LSTM\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (processed DataFrame, normalized DataFrame)\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original\n",
    "    df = df.copy()\n",
    "\n",
    "    # Handle missing values\n",
    "    df = df.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "    # Calculate technical indicators\n",
    "    # RSI\n",
    "    df['RSI'] = vbt.RSI.run(df['Close'], window=14).rsi\n",
    "\n",
    "    # MACD\n",
    "    macd = vbt.MACD.run(df['Close'])\n",
    "    df['MACD'] = macd.macd\n",
    "    df['MACD_Signal'] = macd.signal\n",
    "    df['MACD_Hist'] = macd.hist\n",
    "\n",
    "    # Bollinger Bands\n",
    "    bb = vbt.BBANDS.run(df['Close'], window=20)\n",
    "    df['BB_Upper'] = bb.upper\n",
    "    df['BB_Middle'] = bb.middle\n",
    "    df['BB_Lower'] = bb.lower\n",
    "\n",
    "    # Normalize the data\n",
    "    normalized_df = df.copy()\n",
    "    for column in df.columns:\n",
    "        if column != 'Volume':\n",
    "            normalized_df[column] = (df[column] - df[column].mean()) / df[column].std()\n",
    "        else:\n",
    "            normalized_df[column] = df[column] / df[column].max()\n",
    "\n",
    "    return df, normalized_df\n",
    "\n",
    "# Process the data\n",
    "processed_data, normalized_data = preprocess_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bkHC",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Split the data into training, validation, and test sets\"\"\"\n",
    "def split_data(\n",
    "    df: pd.DataFrame,\n",
    "    train_ratio: float = 0.7,\n",
    "    val_ratio: float = 0.15,\n",
    "    sequence_length: int = 60\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Split the data into training, validation, and test sets\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame with normalized data\n",
    "        train_ratio: Ratio of training data\n",
    "        val_ratio: Ratio of validation data\n",
    "        sequence_length: Length of sequences for LSTM\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (X_train, y_train, X_val, y_val, X_test, y_test)\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Data Splitting ===\")\n",
    "    print(f\"Input DataFrame shape: {df.shape}\")\n",
    "    print(f\"Number of features: {len(df.columns)}\")\n",
    "    print(f\"Features: {df.columns.tolist()}\")\n",
    "\n",
    "    # Calculate split indices\n",
    "    train_size = int(len(df) * train_ratio)\n",
    "    val_size = int(len(df) * val_ratio)\n",
    "\n",
    "    # Create sequences\n",
    "    sequences = []\n",
    "    targets = []\n",
    "\n",
    "    # Ensure we have enough data for sequences\n",
    "    for i in range(len(df) - sequence_length):\n",
    "        # Get sequence of data points\n",
    "        sequence = df.iloc[i:(i + sequence_length)].values\n",
    "        # Get target (next value after sequence)\n",
    "        target = df['Close'].iloc[i + sequence_length]\n",
    "\n",
    "        # Add to lists\n",
    "        sequences.append(sequence)\n",
    "        targets.append(target)\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    sequences = np.array(sequences)\n",
    "    targets = np.array(targets)\n",
    "\n",
    "    print(\"\\n=== Sequence Creation ===\")\n",
    "    print(f\"Sequences shape: {sequences.shape}\")\n",
    "    print(f\"Targets shape: {targets.shape}\")\n",
    "    print(f\"Each sequence contains {sequence_length} time steps\")\n",
    "    print(f\"Each time step has {sequences.shape[2]} features\")\n",
    "\n",
    "    # Split the data\n",
    "    X_train = sequences[:train_size]\n",
    "    y_train = targets[:train_size]\n",
    "    X_val = sequences[train_size:train_size + val_size]\n",
    "    y_val = targets[train_size:train_size + val_size]\n",
    "    X_test = sequences[train_size + val_size:]\n",
    "    y_test = targets[train_size + val_size:]\n",
    "\n",
    "    print(\"\\n=== Split Data Shapes ===\")\n",
    "    print(f\"X_train shape: {X_train.shape}\")\n",
    "    print(f\"y_train shape: {y_train.shape}\")\n",
    "    print(f\"X_val shape: {X_val.shape}\")\n",
    "    print(f\"y_val shape: {y_val.shape}\")\n",
    "    print(f\"X_test shape: {X_test.shape}\")\n",
    "    print(f\"y_test shape: {y_test.shape}\")\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "# Split the data\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = split_data(normalized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lEQa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create PyTorch DataLoaders for training\"\"\"\n",
    "def create_dataloaders(\n",
    "    X_train: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    "    X_val: np.ndarray,\n",
    "    y_val: np.ndarray,\n",
    "    X_test: np.ndarray,\n",
    "    y_test: np.ndarray,\n",
    "    batch_size: int = 32\n",
    ") -> Tuple[torch.utils.data.DataLoader, torch.utils.data.DataLoader, torch.utils.data.DataLoader]:\n",
    "    \"\"\"\n",
    "    Create PyTorch DataLoaders for training\n",
    "\n",
    "    Args:\n",
    "        X_train: Training features\n",
    "        y_train: Training targets\n",
    "        X_val: Validation features\n",
    "        y_val: Validation targets\n",
    "        X_test: Test features\n",
    "        y_test: Test targets\n",
    "        batch_size: Batch size for training\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (train_loader, val_loader, test_loader)\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "    print(\"\\n=== Converting to PyTorch Tensors ===\")\n",
    "    # Convert numpy arrays to PyTorch tensors\n",
    "    X_train = torch.FloatTensor(X_train)\n",
    "    y_train = torch.FloatTensor(y_train)\n",
    "    X_val = torch.FloatTensor(X_val)\n",
    "    y_val = torch.FloatTensor(y_val)\n",
    "    X_test = torch.FloatTensor(X_test)\n",
    "    y_test = torch.FloatTensor(y_test)\n",
    "\n",
    "    print(f\"X_train tensor shape: {X_train.shape}\")\n",
    "    print(f\"y_train tensor shape: {y_train.shape}\")\n",
    "    print(f\"X_val tensor shape: {X_val.shape}\")\n",
    "    print(f\"y_val tensor shape: {y_val.shape}\")\n",
    "    print(f\"X_test tensor shape: {X_test.shape}\")\n",
    "    print(f\"y_test tensor shape: {y_test.shape}\")\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    val_dataset = TensorDataset(X_val, y_val)\n",
    "    test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    print(\"\\n=== DataLoader Information ===\")\n",
    "    print(f\"Train batches: {len(train_loader)}\")\n",
    "    print(f\"Val batches: {len(val_loader)}\")\n",
    "    print(f\"Test batches: {len(test_loader)}\")\n",
    "\n",
    "    # Print shape of first batch\n",
    "    for batch in train_loader:\n",
    "        print(f\"\\nFirst batch shapes:\")\n",
    "        print(f\"Batch X shape: {batch[0].shape}\")\n",
    "        print(f\"Batch y shape: {batch[1].shape}\")\n",
    "        break\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader, val_loader, test_loader = create_dataloaders(\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PKri",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Define the LSTM Generator model\"\"\"\n",
    "class LSTMModel(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        hidden_size: int = 128,\n",
    "        num_layers: int = 2,\n",
    "        dropout: float = 0.2,\n",
    "        output_size: int = 12,  # Changed to match input_size\n",
    "        sequence_length: int = 60\n",
    "    ):\n",
    "        \"\"\"\n",
    "        LSTM-based Generator model\n",
    "\n",
    "        Args:\n",
    "            input_size: Number of input features\n",
    "            hidden_size: Number of hidden units in LSTM\n",
    "            num_layers: Number of LSTM layers\n",
    "            dropout: Dropout rate\n",
    "            output_size: Number of output features (should match input_size)\n",
    "            sequence_length: Length of sequences to generate\n",
    "        \"\"\"\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "        self.lstm = torch.nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "\n",
    "        self.fc = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_size, hidden_size // 2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(dropout),\n",
    "            torch.nn.Linear(hidden_size // 2, output_size * sequence_length)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, sequence_length, input_size)\n",
    "\n",
    "        Returns:\n",
    "            Output tensor of shape (batch_size, sequence_length, input_size)\n",
    "        \"\"\"\n",
    "        # Debug prints for first batch\n",
    "        if x.size(0) == 32:  # First batch\n",
    "            print(\"\\n=== Debug: LSTM Generator ===\")\n",
    "            print(f\"Input shape: {x.shape}\")\n",
    "\n",
    "        # Process through LSTM\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        if x.size(0) == 32:  # First batch\n",
    "            print(f\"LSTM output shape: {lstm_out.shape}\")\n",
    "\n",
    "        # Generate sequence\n",
    "        out = self.fc(lstm_out[:, -1, :])\n",
    "        if x.size(0) == 32:  # First batch\n",
    "            print(f\"FC output shape: {out.shape}\")\n",
    "\n",
    "        # Reshape to (batch_size, sequence_length, input_size)\n",
    "        out = out.view(x.size(0), self.sequence_length, -1)\n",
    "        if x.size(0) == 32:  # First batch\n",
    "            print(f\"Final output shape: {out.shape}\")\n",
    "            print(f\"Output range: [{out.min().item():.4f}, {out.max().item():.4f}]\")\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Xref",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Define the CNN Discriminator model\"\"\"\n",
    "class CNNDiscriminator(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        sequence_length: int,\n",
    "        num_filters: int = 64,\n",
    "        dropout: float = 0.2\n",
    "    ):\n",
    "        \"\"\"\n",
    "        CNN-based Discriminator model\n",
    "\n",
    "        Args:\n",
    "            input_size: Number of input features\n",
    "            sequence_length: Length of input sequences\n",
    "            num_filters: Number of filters in first conv layer\n",
    "            dropout: Dropout rate\n",
    "        \"\"\"\n",
    "        super(CNNDiscriminator, self).__init__()\n",
    "\n",
    "        # First layer: Conv1d expects (batch_size, channels, sequence_length)\n",
    "        self.conv1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv1d(input_size, num_filters, kernel_size=3, padding=1),\n",
    "            torch.nn.BatchNorm1d(num_filters),\n",
    "            torch.nn.LeakyReLU(0.2),\n",
    "            torch.nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        self.conv2 = torch.nn.Sequential(\n",
    "            torch.nn.Conv1d(num_filters, num_filters * 2, kernel_size=3, padding=1),\n",
    "            torch.nn.BatchNorm1d(num_filters * 2),\n",
    "            torch.nn.LeakyReLU(0.2),\n",
    "            torch.nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        self.conv3 = torch.nn.Sequential(\n",
    "            torch.nn.Conv1d(num_filters * 2, num_filters * 4, kernel_size=3, padding=1),\n",
    "            torch.nn.BatchNorm1d(num_filters * 4),\n",
    "            torch.nn.LeakyReLU(0.2),\n",
    "            torch.nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        # Calculate the size of flattened features\n",
    "        self.flatten_size = num_filters * 4 * sequence_length\n",
    "\n",
    "        self.fc = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.flatten_size, 512),\n",
    "            torch.nn.BatchNorm1d(512),\n",
    "            torch.nn.LeakyReLU(0.2),\n",
    "            torch.nn.Dropout(dropout),\n",
    "            torch.nn.Linear(512, 256),\n",
    "            torch.nn.BatchNorm1d(256),\n",
    "            torch.nn.LeakyReLU(0.2),\n",
    "            torch.nn.Dropout(dropout),\n",
    "            torch.nn.Linear(256, 1),\n",
    "            torch.nn.Sigmoid()  # Ensure output is between 0 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, sequence_length, input_size)\n",
    "\n",
    "        Returns:\n",
    "            Output tensor of shape (batch_size, 1) with values between 0 and 1\n",
    "        \"\"\"\n",
    "        # Debug prints for first batch\n",
    "        if x.size(0) == 32:  # First batch\n",
    "            print(\"\\n=== Debug: CNN Discriminator Input ===\")\n",
    "            print(f\"Input shape: {x.shape}\")\n",
    "            print(f\"Input range: [{x.min().item():.4f}, {x.max().item():.4f}]\")\n",
    "\n",
    "        # Check for NaN values in input\n",
    "        if torch.isnan(x).any():\n",
    "            print(\"Warning: NaN values detected in input\")\n",
    "            x = torch.nan_to_num(x, nan=0.0)\n",
    "\n",
    "        # Reshape input for CNN (batch_size, channels, sequence_length)\n",
    "        # First, transpose to (batch_size, input_size, sequence_length)\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        if x.size(0) == 32:  # First batch\n",
    "            print(f\"After transpose shape: {x.shape}\")\n",
    "            print(f\"After transpose range: [{x.min().item():.4f}, {x.max().item():.4f}]\")\n",
    "\n",
    "        # Apply convolutional layers\n",
    "        x = self.conv1(x)\n",
    "        if x.size(0) == 32:  # First batch\n",
    "            print(f\"After conv1 shape: {x.shape}\")\n",
    "            print(f\"After conv1 range: [{x.min().item():.4f}, {x.max().item():.4f}]\")\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        if x.size(0) == 32:  # First batch\n",
    "            print(f\"After conv2 shape: {x.shape}\")\n",
    "            print(f\"After conv2 range: [{x.min().item():.4f}, {x.max().item():.4f}]\")\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        if x.size(0) == 32:  # First batch\n",
    "            print(f\"After conv3 shape: {x.shape}\")\n",
    "            print(f\"After conv3 range: [{x.min().item():.4f}, {x.max().item():.4f}]\")\n",
    "\n",
    "        # Flatten and apply fully connected layers\n",
    "        x = x.view(x.size(0), -1)\n",
    "        if x.size(0) == 32:  # First batch\n",
    "            print(f\"After flatten shape: {x.shape}\")\n",
    "            print(f\"After flatten range: [{x.min().item():.4f}, {x.max().item():.4f}]\")\n",
    "\n",
    "        x = self.fc(x)\n",
    "        if x.size(0) == 32:  # First batch\n",
    "            print(f\"Final output shape: {x.shape}\")\n",
    "            print(f\"Final output range: [{x.min().item():.4f}, {x.max().item():.4f}]\")\n",
    "\n",
    "        # Ensure output is between 0 and 1\n",
    "        x = torch.clamp(x, 0.0, 1.0)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SFPL",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Initialize the models and move them to GPU if available\"\"\"\n",
    "def initialize_models(\n",
    "    input_size: int = X_train.shape[2],\n",
    "    hidden_size: int = 128,\n",
    "    num_layers: int = 2,\n",
    "    dropout: float = 0.2,\n",
    "    sequence_length: int = X_train.shape[1]\n",
    ") -> Tuple[torch.nn.Module, torch.nn.Module]:\n",
    "    \"\"\"\n",
    "    Initialize and configure the Generator and Discriminator models\n",
    "\n",
    "    Args:\n",
    "        input_size: Number of input features\n",
    "        hidden_size: Number of hidden units in LSTM\n",
    "        num_layers: Number of LSTM layers\n",
    "        dropout: Dropout rate\n",
    "        sequence_length: Length of input sequences\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (generator, discriminator)\n",
    "    \"\"\"\n",
    "    # Initialize models\n",
    "    generator = LSTMModel(\n",
    "        input_size=input_size,\n",
    "        hidden_size=hidden_size,\n",
    "        num_layers=num_layers,\n",
    "        dropout=dropout,\n",
    "        sequence_length=sequence_length\n",
    "    )\n",
    "\n",
    "    discriminator = CNNDiscriminator(\n",
    "        input_size=input_size,\n",
    "        sequence_length=sequence_length,\n",
    "        num_filters=64,\n",
    "        dropout=dropout\n",
    "    )\n",
    "\n",
    "    # Move models to GPU if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    generator = generator.to(device)\n",
    "    discriminator = discriminator.to(device)\n",
    "\n",
    "    return generator, discriminator\n",
    "\n",
    "# Initialize models\n",
    "generator, discriminator = initialize_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BYtC",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Define loss functions and optimizers\"\"\"\n",
    "def setup_training_components(\n",
    "    generator: torch.nn.Module,\n",
    "    discriminator: torch.nn.Module,\n",
    "    learning_rate: float = 0.0002,\n",
    "    beta1: float = 0.5\n",
    ") -> Tuple[torch.optim.Optimizer, torch.optim.Optimizer, torch.nn.Module, torch.nn.Module]:\n",
    "    \"\"\"\n",
    "    Set up optimizers and loss functions for training\n",
    "\n",
    "    Args:\n",
    "        generator: Generator model\n",
    "        discriminator: Discriminator model\n",
    "        learning_rate: Learning rate for optimizers\n",
    "        beta1: Beta1 parameter for Adam optimizer\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (g_optimizer, d_optimizer, g_criterion, d_criterion)\n",
    "    \"\"\"\n",
    "    # Optimizers\n",
    "    g_optimizer = torch.optim.Adam(\n",
    "        generator.parameters(),\n",
    "        lr=learning_rate,\n",
    "        betas=(beta1, 0.999)\n",
    "    )\n",
    "\n",
    "    d_optimizer = torch.optim.Adam(\n",
    "        discriminator.parameters(),\n",
    "        lr=learning_rate,\n",
    "        betas=(beta1, 0.999)\n",
    "    )\n",
    "\n",
    "    # Loss functions\n",
    "    g_criterion = torch.nn.MSELoss()\n",
    "    d_criterion = torch.nn.BCELoss()\n",
    "\n",
    "    return g_optimizer, d_optimizer, g_criterion, d_criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RGSE",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Define the training loop for GAN\"\"\"\n",
    "def train_gan(\n",
    "    generator: torch.nn.Module,\n",
    "    discriminator: torch.nn.Module,\n",
    "    train_loader: torch.utils.data.DataLoader,\n",
    "    val_loader: torch.utils.data.DataLoader,\n",
    "    g_optimizer: torch.optim.Optimizer,\n",
    "    d_optimizer: torch.optim.Optimizer,\n",
    "    g_criterion: torch.nn.Module,\n",
    "    d_criterion: torch.nn.Module,\n",
    "    num_epochs: int = 100,\n",
    "    device: torch.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    n_critic: int = 5,  # Number of D updates per G update\n",
    "    lambda_gp: float = 10.0  # Gradient penalty coefficient\n",
    ") -> Tuple[list, list, list, list]:\n",
    "    \"\"\"\n",
    "    Train the GAN model\n",
    "\n",
    "    Args:\n",
    "        generator: Generator model\n",
    "        discriminator: Discriminator model\n",
    "        train_loader: Training data loader\n",
    "        val_loader: Validation data loader\n",
    "        g_optimizer: Generator optimizer\n",
    "        d_optimizer: Discriminator optimizer\n",
    "        g_criterion: Generator loss function\n",
    "        d_criterion: Discriminator loss function\n",
    "        num_epochs: Number of training epochs\n",
    "        device: Device to train on\n",
    "        n_critic: Number of D updates per G update\n",
    "        lambda_gp: Gradient penalty coefficient\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (g_losses, d_losses, val_g_losses, val_d_losses)\n",
    "    \"\"\"\n",
    "    g_losses = []\n",
    "    d_losses = []\n",
    "    val_g_losses = []\n",
    "    val_d_losses = []\n",
    "\n",
    "    def compute_gradient_penalty(discriminator, real_samples, fake_samples):\n",
    "        \"\"\"Compute gradient penalty for WGAN\"\"\"\n",
    "        # Create interpolated samples\n",
    "        alpha = torch.rand(real_samples.size(0), 1, 1).to(device)\n",
    "        interpolates = (alpha * real_samples + (1 - alpha) * fake_samples).requires_grad_(True)\n",
    "\n",
    "        # Get discriminator output for interpolated samples\n",
    "        d_interpolates = discriminator(interpolates)\n",
    "        fake = torch.ones(d_interpolates.size(), requires_grad=False).to(device)\n",
    "\n",
    "        # Compute gradients\n",
    "        gradients = torch.autograd.grad(\n",
    "            outputs=d_interpolates,\n",
    "            inputs=interpolates,\n",
    "            grad_outputs=fake,\n",
    "            create_graph=True,\n",
    "            retain_graph=True,\n",
    "            only_inputs=True\n",
    "        )[0]\n",
    "\n",
    "        # Reshape gradients to (batch_size, -1)\n",
    "        gradients = gradients.reshape(gradients.size(0), -1)\n",
    "\n",
    "        # Compute gradient penalty\n",
    "        gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "        return gradient_penalty\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        g_loss = 0\n",
    "        d_loss = 0\n",
    "\n",
    "        # Training loop\n",
    "        for batch_idx, (real_data, _) in enumerate(train_loader):\n",
    "            real_data = real_data.to(device)\n",
    "            batch_size = real_data.size(0)\n",
    "\n",
    "            # Train Discriminator\n",
    "            d_optimizer.zero_grad()\n",
    "\n",
    "            # Generate fake data\n",
    "            z = torch.randn(batch_size, real_data.size(1), real_data.size(2)).to(device)\n",
    "            fake_data = generator(z)\n",
    "\n",
    "            # Ensure input dimensions are correct for discriminator\n",
    "            # real_data and fake_data should be (batch_size, sequence_length, input_size)\n",
    "            d_real = discriminator(real_data)\n",
    "            d_fake = discriminator(fake_data.detach())  # Detach to avoid G update\n",
    "\n",
    "            # Debug prints for first batch of first epoch\n",
    "            if epoch == 0 and batch_idx == 0:\n",
    "                print(\"\\n=== Debug: Discriminator Outputs ===\")\n",
    "                print(f\"d_real shape: {d_real.shape}\")\n",
    "                print(f\"d_real min: {d_real.min().item():.4f}\")\n",
    "                print(f\"d_real max: {d_real.max().item():.4f}\")\n",
    "                print(f\"d_fake shape: {d_fake.shape}\")\n",
    "                print(f\"d_fake min: {d_fake.min().item():.4f}\")\n",
    "                print(f\"d_fake max: {d_fake.max().item():.4f}\")\n",
    "\n",
    "            # Create labels with proper shape and device\n",
    "            real_labels = torch.ones(batch_size, 1, requires_grad=False).to(device)\n",
    "            fake_labels = torch.zeros(batch_size, 1, requires_grad=False).to(device)\n",
    "\n",
    "            # Compute discriminator losses\n",
    "            d_real_loss = d_criterion(d_real, real_labels)\n",
    "            d_fake_loss = d_criterion(d_fake, fake_labels)\n",
    "\n",
    "            # Gradient penalty\n",
    "            gradient_penalty = compute_gradient_penalty(discriminator, real_data, fake_data)\n",
    "\n",
    "            # Total discriminator loss\n",
    "            d_loss = d_real_loss + d_fake_loss + lambda_gp * gradient_penalty\n",
    "            d_loss.backward()\n",
    "            d_optimizer.step()\n",
    "\n",
    "            # Train Generator\n",
    "            if batch_idx % n_critic == 0:\n",
    "                g_optimizer.zero_grad()\n",
    "\n",
    "                # Generate fake data\n",
    "                z = torch.randn(batch_size, real_data.size(1), real_data.size(2)).to(device)\n",
    "                fake_data = generator(z)\n",
    "\n",
    "                # Generator loss\n",
    "                d_fake = discriminator(fake_data)\n",
    "                g_loss = g_criterion(d_fake, real_labels)  # Generator wants to fool discriminator\n",
    "\n",
    "                g_loss.backward()\n",
    "                g_optimizer.step()\n",
    "\n",
    "        # Validation loop\n",
    "        generator.eval()\n",
    "        discriminator.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            val_g_loss = 0\n",
    "            val_d_loss = 0\n",
    "\n",
    "            for real_data, _ in val_loader:\n",
    "                real_data = real_data.to(device)\n",
    "                batch_size = real_data.size(0)\n",
    "\n",
    "                # Generate fake data\n",
    "                z = torch.randn(batch_size, real_data.size(1), real_data.size(2)).to(device)\n",
    "                fake_data = generator(z)\n",
    "\n",
    "                # Compute validation losses\n",
    "                d_real = discriminator(real_data)\n",
    "                d_fake = discriminator(fake_data)\n",
    "\n",
    "                # Create labels with proper shape and device\n",
    "                real_labels = torch.ones(batch_size, 1, requires_grad=False).to(device)\n",
    "                fake_labels = torch.zeros(batch_size, 1, requires_grad=False).to(device)\n",
    "\n",
    "                val_d_loss += d_criterion(d_real, real_labels)\n",
    "                val_d_loss += d_criterion(d_fake, fake_labels)\n",
    "                val_g_loss += g_criterion(d_fake, real_labels)\n",
    "\n",
    "        # Average validation losses\n",
    "        val_g_loss /= len(val_loader)\n",
    "        val_d_loss /= len(val_loader)\n",
    "\n",
    "        # Store losses\n",
    "        g_losses.append(g_loss.item())\n",
    "        d_losses.append(d_loss.item())\n",
    "        val_g_losses.append(val_g_loss.item())\n",
    "        val_d_losses.append(val_d_loss.item())\n",
    "\n",
    "        # Print progress\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "                  f'G_loss: {g_loss.item():.4f}, '\n",
    "                  f'D_loss: {d_loss.item():.4f}, '\n",
    "                  f'Val_G_loss: {val_g_loss.item():.4f}, '\n",
    "                  f'Val_D_loss: {val_d_loss.item():.4f}')\n",
    "\n",
    "    return g_losses, d_losses, val_g_losses, val_d_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Kclp",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Define the PPO training loop\"\"\"\n",
    "class PPOTrainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        generator: torch.nn.Module,\n",
    "        discriminator: torch.nn.Module,\n",
    "        learning_rate: float = 0.0003,\n",
    "        gamma: float = 0.99,\n",
    "        epsilon: float = 0.2,\n",
    "        c1: float = 1.0,\n",
    "        c2: float = 0.01,\n",
    "        batch_size: int = 64,\n",
    "        n_epochs: int = 4\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize PPO trainer\n",
    "\n",
    "        Args:\n",
    "            generator: Generator model\n",
    "            discriminator: Discriminator model\n",
    "            learning_rate: Learning rate\n",
    "            gamma: Discount factor\n",
    "            epsilon: PPO clipping parameter\n",
    "            c1: Value function coefficient\n",
    "            c2: Entropy coefficient\n",
    "            batch_size: Batch size for training\n",
    "            n_epochs: Number of epochs per update\n",
    "        \"\"\"\n",
    "        self.generator = generator\n",
    "        self.discriminator = discriminator\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.c1 = c1\n",
    "        self.c2 = c2\n",
    "        self.batch_size = batch_size\n",
    "        self.n_epochs = n_epochs\n",
    "\n",
    "        # Optimizers\n",
    "        self.g_optimizer = torch.optim.Adam(generator.parameters(), lr=learning_rate)\n",
    "        self.d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=learning_rate)\n",
    "\n",
    "        # Memory buffers\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.values = []\n",
    "        self.log_probs = []\n",
    "\n",
    "    def select_action(self, state: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Select action using current policy\"\"\"\n",
    "        with torch.no_grad():\n",
    "            action = self.generator(state)\n",
    "            value = self.discriminator(state)\n",
    "            log_prob = torch.distributions.Normal(action, 1.0).log_prob(action)\n",
    "\n",
    "        return action, value, log_prob\n",
    "\n",
    "    def update(self) -> Tuple[float, float]:\n",
    "        \"\"\"Update policy using PPO\"\"\"\n",
    "        # Convert lists to tensors\n",
    "        states = torch.stack(self.states)\n",
    "        actions = torch.stack(self.actions)\n",
    "        old_log_probs = torch.stack(self.log_probs)\n",
    "\n",
    "        # Calculate advantages\n",
    "        rewards = torch.tensor(self.rewards)\n",
    "        values = torch.tensor(self.values)\n",
    "        advantages = rewards - values\n",
    "\n",
    "        # Normalize advantages\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "        # PPO update\n",
    "        for _ in range(self.n_epochs):\n",
    "            # Generate new action probabilities\n",
    "            new_actions, new_values, new_log_probs = self.select_action(states)\n",
    "\n",
    "            # Calculate ratio\n",
    "            ratio = torch.exp(new_log_probs - old_log_probs)\n",
    "\n",
    "            # Calculate surrogate losses\n",
    "            surr1 = ratio * advantages\n",
    "            surr2 = torch.clamp(ratio, 1.0 - self.epsilon, 1.0 + self.epsilon) * advantages\n",
    "\n",
    "            # Calculate policy loss\n",
    "            policy_loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "            # Calculate value loss\n",
    "            value_loss = torch.nn.MSELoss()(new_values, rewards)\n",
    "\n",
    "            # Calculate entropy loss\n",
    "            entropy_loss = -0.01 * torch.mean(new_log_probs)\n",
    "\n",
    "            # Total loss\n",
    "            total_loss = policy_loss + self.c1 * value_loss + self.c2 * entropy_loss\n",
    "\n",
    "            # Update generator\n",
    "            self.g_optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            self.g_optimizer.step()\n",
    "\n",
    "            # Update discriminator\n",
    "            self.d_optimizer.zero_grad()\n",
    "            value_loss.backward()\n",
    "            self.d_optimizer.step()\n",
    "\n",
    "        # Clear memory\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.values = []\n",
    "        self.log_probs = []\n",
    "\n",
    "        return policy_loss.item(), value_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emfo",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Initialize and start training\"\"\"\n",
    "# Set up training components\n",
    "g_optimizer, d_optimizer, g_criterion, d_criterion = setup_training_components(\n",
    "    generator, discriminator\n",
    ")\n",
    "\n",
    "# Train GAN\n",
    "g_losses, d_losses, val_g_losses, val_d_losses = train_gan(\n",
    "    generator=generator,\n",
    "    discriminator=discriminator,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    g_optimizer=g_optimizer,\n",
    "    d_optimizer=d_optimizer,\n",
    "    g_criterion=g_criterion,\n",
    "    d_criterion=d_criterion\n",
    ")\n",
    "\n",
    "# Initialize PPO trainer\n",
    "ppo_trainer = PPOTrainer(generator, discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Hstk",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Phase 4: PPO Training Implementation\"\"\"\n",
    "class TradingEnvironment:\n",
    "    def __init__(self, data: pd.DataFrame, sequence_length: int = 60):\n",
    "        \"\"\"\n",
    "        Trading environment for PPO training\n",
    "\n",
    "        Args:\n",
    "            data: DataFrame with OHLCV data and technical indicators\n",
    "            sequence_length: Length of sequences for state representation\n",
    "        \"\"\"\n",
    "        # Create a copy of the data to avoid modifying the original\n",
    "        self.data = data.copy()\n",
    "\n",
    "        # Calculate technical indicators if not already present\n",
    "        if 'RSI' not in self.data.columns:\n",
    "            self.data['RSI'] = vbt.RSI.run(self.data['Close'], window=14).rsi\n",
    "            macd = vbt.MACD.run(self.data['Close'])\n",
    "            self.data['MACD'] = macd.macd\n",
    "            self.data['MACD_Signal'] = macd.signal\n",
    "            self.data['MACD_Hist'] = macd.hist\n",
    "            bb = vbt.BBANDS.run(self.data['Close'], window=20)\n",
    "            self.data['BB_Upper'] = bb.upper\n",
    "            self.data['BB_Middle'] = bb.middle\n",
    "            self.data['BB_Lower'] = bb.lower\n",
    "\n",
    "        # Normalize the data\n",
    "        for column in self.data.columns:\n",
    "            if column != 'Volume':\n",
    "                self.data[column] = (self.data[column] - self.data[column].mean()) / self.data[column].std()\n",
    "            else:\n",
    "                self.data[column] = self.data[column] / self.data[column].max()\n",
    "\n",
    "        self.sequence_length = sequence_length\n",
    "        self.current_step = sequence_length\n",
    "        self.max_steps = len(data) - sequence_length\n",
    "        self.initial_balance = 10000.0\n",
    "        self.balance = self.initial_balance\n",
    "        self.position = 0\n",
    "        self.trades = []\n",
    "\n",
    "        print(\"\\n=== Trading Environment Initialization ===\")\n",
    "        print(f\"Number of features: {len(self.data.columns)}\")\n",
    "        print(f\"Features: {self.data.columns.tolist()}\")\n",
    "        print(f\"Sequence length: {sequence_length}\")\n",
    "        print(f\"Max steps: {self.max_steps}\")\n",
    "\n",
    "    def reset(self) -> torch.Tensor:\n",
    "        \"\"\"Reset the environment\"\"\"\n",
    "        self.current_step = self.sequence_length\n",
    "        self.balance = self.initial_balance\n",
    "        self.position = 0\n",
    "        self.trades = []\n",
    "        return self._get_state()\n",
    "\n",
    "    def _get_state(self) -> torch.Tensor:\n",
    "        \"\"\"Get current state representation\"\"\"\n",
    "        state_data = self.data.iloc[self.current_step-self.sequence_length:self.current_step].values\n",
    "        return torch.FloatTensor(state_data)\n",
    "\n",
    "    def step(self, action: float) -> Tuple[torch.Tensor, float, bool, dict]:\n",
    "        \"\"\"\n",
    "        Execute one step in the environment\n",
    "\n",
    "        Args:\n",
    "            action: Trading action (-1: sell, 0: hold, 1: buy)\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (next_state, reward, done, info)\n",
    "        \"\"\"\n",
    "        current_price = self.data['Close'].iloc[self.current_step]\n",
    "        next_price = self.data['Close'].iloc[self.current_step + 1]\n",
    "\n",
    "        # Calculate price change\n",
    "        price_change = (next_price - current_price) / current_price\n",
    "\n",
    "        # Execute action\n",
    "        reward = 0\n",
    "        if action > 0 and self.position <= 0:  # Buy signal\n",
    "            if self.position < 0:  # Close short position\n",
    "                reward += -price_change * abs(self.position)\n",
    "            self.position = 1\n",
    "        elif action < 0 and self.position >= 0:  # Sell signal\n",
    "            if self.position > 0:  # Close long position\n",
    "                reward += price_change * self.position\n",
    "            self.position = -1\n",
    "\n",
    "        # Calculate reward based on position and price change\n",
    "        reward += price_change * self.position\n",
    "\n",
    "        # Record trade\n",
    "        self.trades.append({\n",
    "            'step': self.current_step,\n",
    "            'action': action,\n",
    "            'position': self.position,\n",
    "            'price': current_price,\n",
    "            'reward': reward\n",
    "        })\n",
    "\n",
    "        # Update step\n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= self.max_steps - 1\n",
    "\n",
    "        # Calculate info\n",
    "        info = {\n",
    "            'balance': self.balance,\n",
    "            'position': self.position,\n",
    "            'trades': self.trades\n",
    "        }\n",
    "\n",
    "        return self._get_state(), reward, done, info\n",
    "\n",
    "def train_ppo(\n",
    "    ppo_trainer: PPOTrainer,\n",
    "    env: TradingEnvironment,\n",
    "    num_episodes: int = 1000,\n",
    "    max_steps: int = 1000,\n",
    "    gamma: float = 0.99,\n",
    "    gae_lambda: float = 0.95,\n",
    "    clip_epsilon: float = 0.2,\n",
    "    c1: float = 1.0,\n",
    "    c2: float = 0.01,\n",
    "    batch_size: int = 64,\n",
    "    n_epochs: int = 4\n",
    ") -> Tuple[list, list]:\n",
    "    \"\"\"\n",
    "    Train the trading strategy using PPO\n",
    "\n",
    "    Args:\n",
    "        ppo_trainer: PPO trainer instance\n",
    "        env: Trading environment\n",
    "        num_episodes: Number of training episodes\n",
    "        max_steps: Maximum steps per episode\n",
    "        gamma: Discount factor\n",
    "        gae_lambda: GAE lambda parameter\n",
    "        clip_epsilon: PPO clipping parameter\n",
    "        c1: Value function coefficient\n",
    "        c2: Entropy coefficient\n",
    "        batch_size: Batch size for training\n",
    "        n_epochs: Number of epochs per update\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (episode_rewards, episode_lengths)\n",
    "    \"\"\"\n",
    "    episode_rewards = []\n",
    "    episode_lengths = []\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        episode_length = 0\n",
    "\n",
    "        # Collect trajectory\n",
    "        while episode_length < max_steps:\n",
    "            # Select action\n",
    "            action, value, log_prob = ppo_trainer.select_action(state)\n",
    "\n",
    "            # Take action in environment\n",
    "            next_state, reward, done, info = env.step(action.item())\n",
    "\n",
    "            # Store transition\n",
    "            ppo_trainer.states.append(state)\n",
    "            ppo_trainer.actions.append(action)\n",
    "            ppo_trainer.rewards.append(reward)\n",
    "            ppo_trainer.values.append(value)\n",
    "            ppo_trainer.log_probs.append(log_prob)\n",
    "\n",
    "            # Update state and episode info\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            episode_length += 1\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # Update policy\n",
    "        policy_loss, value_loss = ppo_trainer.update()\n",
    "\n",
    "        # Record episode statistics\n",
    "        episode_rewards.append(episode_reward)\n",
    "        episode_lengths.append(episode_length)\n",
    "\n",
    "        # Print progress\n",
    "        if (episode + 1) % 10 == 0:\n",
    "            print(f'Episode [{episode+1}/{num_episodes}], '\n",
    "                  f'Reward: {episode_reward:.2f}, '\n",
    "                  f'Length: {episode_length}, '\n",
    "                  f'Policy Loss: {policy_loss:.4f}, '\n",
    "                  f'Value Loss: {value_loss:.4f}')\n",
    "\n",
    "    return episode_rewards, episode_lengths\n",
    "\n",
    "# Initialize environment and start training\n",
    "env = TradingEnvironment(processed_data)  # Use processed_data instead of raw data\n",
    "episode_rewards, episode_lengths = train_ppo(\n",
    "    ppo_trainer=ppo_trainer,\n",
    "    env=env,\n",
    "    num_episodes=1000,\n",
    "    max_steps=1000\n",
    ")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}

